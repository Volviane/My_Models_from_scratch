{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_train = pd.read_csv(\"Data/drugLibTrain_raw.tsv\",delimiter='\\t',encoding='utf-8')\n",
    "drug_test = pd.read_csv(\"Data/drugLibTest_raw.tsv\",delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "drug=pd.concat([drug_train,drug_test], axis=0, join='outer',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4143 entries, 0 to 1035\n",
      "Data columns (total 9 columns):\n",
      "Unnamed: 0           4143 non-null int64\n",
      "urlDrugName          4143 non-null object\n",
      "rating               4143 non-null int64\n",
      "effectiveness        4143 non-null object\n",
      "sideEffects          4143 non-null object\n",
      "condition            4142 non-null object\n",
      "benefitsReview       4143 non-null object\n",
      "sideEffectsReview    4141 non-null object\n",
      "commentsReview       4135 non-null object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 323.7+ KB\n"
     ]
    }
   ],
   "source": [
    "drug.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4132 entries, 0 to 1035\n",
      "Data columns (total 9 columns):\n",
      "Unnamed: 0           4132 non-null int64\n",
      "urlDrugName          4132 non-null object\n",
      "rating               4132 non-null int64\n",
      "effectiveness        4132 non-null object\n",
      "sideEffects          4132 non-null object\n",
      "condition            4132 non-null object\n",
      "benefitsReview       4132 non-null object\n",
      "sideEffectsReview    4132 non-null object\n",
      "commentsReview       4132 non-null object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 322.8+ KB\n"
     ]
    }
   ],
   "source": [
    "drug=drug.dropna(axis=0,how='any')\n",
    "drug.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text(max_words, data):\n",
    "    \n",
    "    data['Alltext'] = data['benefitsReview']#+' '+data['benefitsReview']\\\n",
    "                #+' ' +data['commentsReview']+''+data['sideEffectsReview']\n",
    "    data['Alltext'] = data['Alltext'].apply(lambda x : str(x))\n",
    "    texts_tr = data.Alltext\n",
    "\n",
    "    tokenizer_tr = Tokenizer(num_words=max_words)\n",
    "    tokenizer_tr.fit_on_texts(texts_tr)\n",
    "    X = tokenizer_tr.texts_to_matrix(texts_tr, mode='binary')\n",
    "    y=data['rating'].values - 1\n",
    "    word_index = tokenizer_tr.word_index\n",
    "    print('Found {} unique tokens.'.format( len(set(word_index))))\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9108 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "X,y = convert_text(500,drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3305, 500)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Discriminant Analysis(GDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiscriminantAnalysis:\n",
    "    def __init__(self,\n",
    "                 epsilon=2e-1\n",
    "                ):\n",
    "        \n",
    "        self.epsilon=epsilon\n",
    "        \n",
    "    def ph(self,y):\n",
    "        phi=[]\n",
    "        for k in range(self.nb_class):\n",
    "            phi.append((np.sum(y==k))/len(y))\n",
    "        return phi   \n",
    "    \n",
    "    def mu_computed(self,X,y):\n",
    "        list_of_mu=[]\n",
    "        for k in range(self.nb_class):\n",
    "            list_of_mu.append((np.sum(X[y==k],axis=0))/np.sum(y==k))\n",
    "        return list_of_mu\n",
    "    \n",
    "    def covariance(self,X,y):\n",
    "        sigma= np.zeros((len(X),len(X)))\n",
    "        M = np.zeros_like(X)\n",
    "        for k in range(self.nb_class):\n",
    "            M[y== k]=self.list_mu[k]\n",
    "        return (((X-M).T)@(X-M))/len(y)\n",
    "    \n",
    "    def prob_class(self,X):\n",
    "        half_len = X.shape[1]/2\n",
    "        det_cvar = np.sqrt(np.linalg.det(self.cvar + (self.epsilon*np.eye(len(self.cvar)))))\n",
    "        inv_cvar = np.linalg.inv(self.cvar + (self.epsilon*np.eye(len(self.cvar))))\n",
    "        list_prob=[]\n",
    "        for k in range(self.nb_class):\n",
    "            A=((X - self.list_mu[k])@inv_cvar)\n",
    "            #print('dima',A)\n",
    "            B=(X-self.list_mu[k])\n",
    "            #print('dimb',B)\n",
    "            C=np.sum(np.multiply(A,B),axis=1)\n",
    "            e = np.exp( -0.5*C)\n",
    "            p = (2*np.pi)**half_len\n",
    "            #print('cccc',det_cvar)\n",
    "            list_prob.append(np.exp( -0.5*C)*(1/((2*np.pi)**half_len)*np.sqrt(det_cvar)))\n",
    "            #print(list_prob)\n",
    "        return list_prob\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        self.nb_class = len(np.unique(y))\n",
    "        self.classes = np.unique(y)\n",
    "        self.fi = self.ph(y)\n",
    "        self.list_mu = self.mu_computed(X,y)\n",
    "        self.cvar = self.covariance(X, y)\n",
    "        \n",
    "    def predict(self, X):        \n",
    "        proby=self.fi\n",
    "        #print(np.array(proby).shape)\n",
    "        list_probclass= np.array(self.prob_class(X)).T\n",
    "        predict_prob= list_probclass*proby\n",
    "        #print(list_probclass)\n",
    "        #print(predict_prob[0:2])\n",
    "        if 0 in self.classes:\n",
    "            return np.argmax(np.array(predict_prob),axis=1) \n",
    "        else:\n",
    "            return np.argmax(np.array(predict_prob),axis=1) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDA = GaussianDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDA.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cccc 3.3092988843132725e-160\n",
      "cccc 3.3092988843132725e-160\n",
      "cccc 3.3092988843132725e-160\n",
      "cccc 3.3092988843132725e-160\n",
      "cccc 3.3092988843132725e-160\n",
      "cccc 3.3092988843132725e-160\n",
      "cccc 3.3092988843132725e-160\n",
      "cccc 3.3092988843132725e-160\n",
      "cccc 3.3092988843132725e-160\n",
      "cccc 3.3092988843132725e-160\n"
     ]
    }
   ],
   "source": [
    "prediction=GDA.predict(X_test)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.318016928657798"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(prediction==y_test+1)/len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.755743651753328"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred==y_test)/len(y_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb #ipython debogeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNaiveBayes:\n",
    "    '''\n",
    "    multivariate Bernoulli Naive bayes\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def phiy(self,y): \n",
    "        phi=[]\n",
    "        for k in self.classes:\n",
    "            phi.append(np.sum(y==k)/len(y))\n",
    "        return phi\n",
    "    \n",
    "    \n",
    "    def phix1y(self,X,y):\n",
    "        phi_list=[]\n",
    "        for k in self.classes:\n",
    "            phix=[]\n",
    "            for i in range(X.shape[1]):\n",
    "                X_i=X[:,i]\n",
    "                select_y=X_i[y==k]\n",
    "                phix.append((np.sum(select_y==1)+1)/(np.sum(y==k)+self.nb_class))\n",
    "            phi_list.append(phix)\n",
    "        return phi_list\n",
    "    \n",
    "    def phix0y(self,X,y):\n",
    "        phi_list=[]\n",
    "        for k in self.classes:\n",
    "            phix=[]\n",
    "            for i in range(X.shape[1]):\n",
    "                X_i=X[:,i]\n",
    "                select_y=X_i[y==k]\n",
    "                phix.append((np.sum(select_y==0)+1)/(np.sum(y==k)+self.nb_class))\n",
    "            phi_list.append(phix)\n",
    "        return phi_list\n",
    "    \n",
    "    def probabxy(self,X):\n",
    "        \n",
    "        probxyk=[]\n",
    "        arr1=np.zeros_like(X)\n",
    "        for k in range(self.nb_class):\n",
    "            probxy1=[]\n",
    "            for i in range(X.shape[0]):\n",
    "                X_i=X[i,:]\n",
    "                prob=1\n",
    "                for j in range(len(X_i)):\n",
    "\n",
    "                    if X_i[j]==1:\n",
    "                        prob *=  self.fix1y[int(k)][j]\n",
    "                    else:\n",
    "                        \n",
    "                        prob *= self.fix0y[int(k)][j]\n",
    "                    arr1[i,j]=prob\n",
    "                probxy1.append(prob)\n",
    "            probxyk.append(probxy1)\n",
    "        return  probxyk \n",
    "\n",
    "    def train(self, X,y):\n",
    "        self.nb_class = len(np.unique(y))\n",
    "        self.classes = np.unique(y)\n",
    "        self.fiy = self.phiy(y)\n",
    "        self.fix1y = np.array(self.phix1y(X,y))\n",
    "        self.fix0y = np.array(self.phix0y(X,y))\n",
    "        \n",
    "    def predict(self,X) :\n",
    "        proby=self.fiy\n",
    "        list_probclass= self.probabxy(X)\n",
    "        \n",
    "        predict_prob=[]\n",
    "        predict_prob=np.array(list_probclass).T*proby\n",
    "        if 0 in self.classes:\n",
    "            return  np.argmax(predict_prob,axis=1)\n",
    "        else:\n",
    "            return  np.argmax(predict_prob,axis=1)+1         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = BernoulliNaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.114873035066505"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.sum(prediction==y_test+1)/len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.020556227327692"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.sum(prediction==y_test)/len(y_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    '''\n",
    "    Logistic regression for multi class\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 lr=0.0001, \n",
    "                 epoch=1,\n",
    "                 tolerence=10e-8,\n",
    "                 minibatchsize=30,\n",
    "                 lambd=0):\n",
    "        self.lr=lr\n",
    "        self.epoch=epoch\n",
    "        self.tolerence=tolerence\n",
    "        self.minibatchsize=minibatchsize\n",
    "        self.lambd = lambd\n",
    "        \n",
    "    def softmax(self,X,theta):\n",
    "        z = X@theta\n",
    "        z -= np.max(z)\n",
    "        return np.exp(z)/np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    \n",
    "    # compute the grandient     \n",
    "    def get_gradient(self, X, y, theta):\n",
    "        n, _ = X.shape\n",
    "        theta_wo_bias = theta.copy()\n",
    "        theta_wo_bias[0, :] = 0\n",
    "        \n",
    "        grad = (-1 / n) * X.T@(y - self.softmax(X, theta)) + self.lambd * theta_wo_bias\n",
    "        return grad\n",
    "     \n",
    "    #compute the loss\n",
    "    def loss(self, X, y, theta):\n",
    "        n, d = X.shape\n",
    "        y_hat = self.softmax(X, theta)\n",
    " \n",
    "        temp_theta = theta.copy()\n",
    "        temp_theta[0, :] = 0 # Not including bias in regularization\n",
    "       \n",
    "        loss = (-1 / n) * np.sum(y * np.log(y_hat)) \\\n",
    "                + (self.lambd/2)*np.sum(temp_theta*temp_theta)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def one_hot_encode(self, y):\n",
    "        '''\n",
    "        one hot encode the target\n",
    "        to be able to work with softmax\n",
    "        '''\n",
    "        n = len(y)\n",
    "        # Turn y into one-hot-labels if number of classes is greater than 2\n",
    "        y_encode = np.zeros((n, self.nb_classes))\n",
    "        y_encode[range(n), y] = 1 #numpy advanced indexing\n",
    "        y = y_encode\n",
    "        return y\n",
    "   \n",
    "    #mini batch gradient descent        \n",
    "    def fit(self,X,y):\n",
    "        '''\n",
    "        This method take tha data, \n",
    "        use minibactch gradient descent to update the weight\n",
    "        and compute the coresponding cost\n",
    "        '''\n",
    "        #make a copy of our data\n",
    "        X_copy=X.copy()\n",
    "        y_copy=y.copy()\n",
    "        self.classes = np.unique(y_copy)\n",
    "        self.nb_classes = len(self.classes)\n",
    "        \n",
    "         #add the intercept column\n",
    "        intercept=np.ones((X_copy.shape[0],1))\n",
    "        X_copy=np.concatenate((intercept,X_copy),axis=1)\n",
    "        \n",
    "        \n",
    "        #initialise the weight\n",
    "        self.theta=np.zeros((X_copy.shape[1], self.nb_classes))\n",
    "        \n",
    "        # One-hot encode y\n",
    "        y_copy = self.one_hot_encode(y_copy)\n",
    "       \n",
    "        diff=1\n",
    "        current_iter=1\n",
    "        #number of minibacth\n",
    "        minibatch = int(len(X_copy)/self.minibatchsize)\n",
    "        while (diff >= self.tolerence) and (current_iter<self.epoch) :\n",
    "            prev_theta=self.theta.copy()\n",
    "            random_vector=np.random.permutation(X_copy.shape[0])\n",
    "            X_cop=X_copy[random_vector]\n",
    "            y_cop=y_copy[random_vector]\n",
    "            for j in range(minibatch):\n",
    "                X_=X_cop[j*self.minibatchsize:(j+1)*self.minibatchsize]\n",
    "                y_=y_cop[j*self.minibatchsize:(j+1)*self.minibatchsize]\n",
    "                #compute the gradient\n",
    "                grad = self.get_gradient(X_, y_, self.theta)\n",
    "                #update the weight\n",
    "                self.theta = self.theta - self.lr*grad \n",
    "            cur_theta=self.theta.copy()\n",
    "            diff=np.linalg.norm(prev_theta - cur_theta)\n",
    "            current_iter+=1\n",
    "            #print('the loss function is ',self.loss(X_,y_,self.theta))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns probability of predictions.\n",
    "        \"\"\"\n",
    "        X_copy = X.copy()\n",
    "        intercept=np.ones((X_copy.shape[0],1))\n",
    "        X_copy=np.concatenate((intercept,X_copy), axis=1)\n",
    "\n",
    "        return self.softmax(X_copy, self.theta) \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "            gives the prediction using the softmax function\n",
    "        '''\n",
    "        prob = self.predict_proba(X)\n",
    "        y_predict = np.argmax(prob, axis=1)\n",
    "        return y_predict     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = SoftmaxRegression(lr=0.1, \n",
    "                           epoch=300,\n",
    "                           tolerence=10e-8,\n",
    "                           minibatchsize=50,\n",
    "                           lambd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=logReg.predict(X_test)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.18379685610641"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predict == y_test+1)/ len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(size,data):\n",
    "    max_words=500\n",
    "    data_copy=data.copy()\n",
    "    n, _ = data_copy.shape\n",
    "    #print(n)\n",
    "    size_data = int((n*size) /100)\n",
    "    #print(size_data)\n",
    "    split = size_data-20\n",
    "    drug_data=data_copy[ :size_data]\n",
    "    print(drug_data.shape)\n",
    "    X,y =convert_text(max_words, drug_data)\n",
    "    print (X.shape)\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "    X_train = X[:split]\n",
    "    y_train =y[:split]\n",
    "    X_test = X[split:]\n",
    "    y_test =y[split:]  \n",
    "    \n",
    "    GDA = GaussianDiscriminantAnalysis()\n",
    "    NB = BernoulliNaiveBayes()\n",
    "    logReg = SoftmaxRegression(lr=0.1, \n",
    "                           epoch=300,\n",
    "                           tolerence=10e-8,\n",
    "                           minibatchsize=50,\n",
    "                           lambd=0)\n",
    "    \n",
    "    \n",
    "    GDA.train(X_train,y_train)\n",
    "    predictionGDA = GDA.predict(X_test) +1\n",
    "\n",
    "    print(y_test.shape)\n",
    "    print('The accuracy for GDA is ', np.sum(predictionGDA == (y_test+1))/ len(y_test)*100)\n",
    "    \n",
    "    NB.train(X_train,y_train)\n",
    "    predictionNB = NB.predict(X_test) +1\n",
    "    print('The accuracy for NB is ', np.sum(predictionNB == (y_test+1))/ len(y_test)*100)\n",
    "    \n",
    "    logReg.fit(X_train,y_train)\n",
    "    predictionlogReg = logReg.predict(X_test) + 1\n",
    "    print('The accuracy for logReg is ', np.sum(predictionlogReg == (y_test+1))/ len(y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(413, 10)\n",
      "Found 2741 unique tokens.\n",
      "(413, 500)\n",
      "(20,)\n",
      "The accuracy for GDA is  35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for NB is  20.0\n",
      "The accuracy for logReg is  40.0\n"
     ]
    }
   ],
   "source": [
    "test(10,drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1239, 10)\n",
      "Found 4937 unique tokens.\n",
      "(1239, 500)\n",
      "(20,)\n",
      "The accuracy for GDA is  30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for NB is  35.0\n",
      "The accuracy for logReg is  30.0\n"
     ]
    }
   ],
   "source": [
    "test(30,drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2479, 10)\n",
      "Found 7117 unique tokens.\n",
      "(2479, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "The accuracy for GDA is  35.0\n",
      "The accuracy for NB is  25.0\n",
      "The accuracy for logReg is  45.0\n"
     ]
    }
   ],
   "source": [
    "test(60,drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4132, 10)\n",
      "Found 9108 unique tokens.\n",
      "(4132, 500)\n",
      "(20,)\n",
      "The accuracy for GDA is  30.0\n",
      "The accuracy for NB is  20.0\n",
      "The accuracy for logReg is  25.0\n"
     ]
    }
   ],
   "source": [
    "test(100,drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
