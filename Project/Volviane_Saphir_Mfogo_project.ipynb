{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative and Discriminative Modelling Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import the needed package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#package for converting the data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#package to split the data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the drugLib data\n",
    "drug_train = pd.read_csv(\"Data/drugLibTrain_raw.tsv\",delimiter='\\t',encoding='utf-8')\n",
    "drug_test = pd.read_csv(\"Data/drugLibTest_raw.tsv\",delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "# combine the train and test to have one data\n",
    "drug=pd.concat([drug_train,drug_test], axis=0, join='outer',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4143 entries, 0 to 1035\n",
      "Data columns (total 9 columns):\n",
      "Unnamed: 0           4143 non-null int64\n",
      "urlDrugName          4143 non-null object\n",
      "rating               4143 non-null int64\n",
      "effectiveness        4143 non-null object\n",
      "sideEffects          4143 non-null object\n",
      "condition            4142 non-null object\n",
      "benefitsReview       4143 non-null object\n",
      "sideEffectsReview    4141 non-null object\n",
      "commentsReview       4135 non-null object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 323.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#get some informations about the data\n",
    "drug.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When we look at the data we can see that some value are missing, \n",
    "Since just missing data are just in the few observation I decided to drop those obsevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4132 entries, 0 to 1035\n",
      "Data columns (total 9 columns):\n",
      "Unnamed: 0           4132 non-null int64\n",
      "urlDrugName          4132 non-null object\n",
      "rating               4132 non-null int64\n",
      "effectiveness        4132 non-null object\n",
      "sideEffects          4132 non-null object\n",
      "condition            4132 non-null object\n",
      "benefitsReview       4132 non-null object\n",
      "sideEffectsReview    4132 non-null object\n",
      "commentsReview       4132 non-null object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 322.8+ KB\n"
     ]
    }
   ],
   "source": [
    "drug=drug.dropna(axis=0,how='any')\n",
    "drug.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As preprocessing step, we convert our text data into number(binary)\n",
    "def convert_text(max_words, data):\n",
    "    '''\n",
    "    This function takes the maximum number of word we want to convert\n",
    "    and the data, then use Tokenizer to convert them into binary value (0,1)\n",
    "    \n",
    "    It return the X (faeture) and y(target) of the data\n",
    "    '''\n",
    "    \n",
    "    data['Alltext'] = data['benefitsReview']#+' '+data['benefitsReview']\\\n",
    "                #+' ' +data['commentsReview']+''+data['sideEffectsReview']\n",
    "    data['Alltext'] = data['Alltext'].apply(lambda x : str(x))\n",
    "    texts_tr = data.Alltext\n",
    "\n",
    "    tokenizer_tr = Tokenizer(num_words=max_words)\n",
    "    tokenizer_tr.fit_on_texts(texts_tr)\n",
    "    X = tokenizer_tr.texts_to_matrix(texts_tr, mode='binary')\n",
    "    y=data['rating'].values - 1\n",
    "    word_index = tokenizer_tr.word_index\n",
    "    #print('Found {} unique tokens.'.format( len(set(word_index))))\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = convert_text(500,drug)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 -- we write a python class to implement Gaussian Discriminant Analysis(GDA) ,  class to implement Naive Bayes (NB) algorithm from scratch  and another class to implement Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Discriminant Analysis(GDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiscriminantAnalysis:\n",
    "    \n",
    "    '''\n",
    "    This is a generative (we will modelling the data)  learning algorithm in which we assume \n",
    "    p(X|y) is distributed according to a multivariate Normal distribution and p(y) \n",
    "    is distributed according to Bernoulli.\n",
    "    \n",
    "    We will train our model in this class using the function 'train' that will \n",
    "    takes the trainning data and train by computing the parameters.\n",
    "    \n",
    "    So the function predict will takes the test (unseen) data , classify and return \n",
    "    the prediction\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 epsilon=2e-1\n",
    "                ):\n",
    "        \n",
    "        self.epsilon=epsilon\n",
    "    #compute phi for each class    \n",
    "    def ph(self,y):\n",
    "        phi=[]\n",
    "        for k in range(self.nb_class):\n",
    "            phi.append((np.sum(y==k))/len(y))\n",
    "        return phi \n",
    "    \n",
    "    #compute mu for each class\n",
    "    def mu_computed(self,X,y):\n",
    "        list_of_mu=[]\n",
    "        for k in range(self.nb_class):\n",
    "            list_of_mu.append((np.sum(X[y==k],axis=0))/np.sum(y==k))\n",
    "        return list_of_mu\n",
    "    \n",
    "    # compute the covariance matrix\n",
    "    def covariance(self,X,y):\n",
    "        sigma= np.zeros((len(X),len(X)))\n",
    "        M = np.zeros_like(X)\n",
    "        for k in range(self.nb_class):\n",
    "            M[y== k]=self.list_mu[k]\n",
    "        return (((X-M).T)@(X-M))/len(y)\n",
    "    \n",
    "    \n",
    "    def prob_class(self,X):\n",
    "        '''\n",
    "        compute the probability of x given a class \n",
    "                (This is the function that help us modelling the data)\n",
    "        '''\n",
    "        half_len = X.shape[1]/2\n",
    "        det_cvar = np.sqrt(np.linalg.det(self.cvar + (self.epsilon*np.eye(len(self.cvar)))))\n",
    "        inv_cvar = np.linalg.inv(self.cvar + (self.epsilon*np.eye(len(self.cvar))))\n",
    "        list_prob=[]\n",
    "        for k in range(self.nb_class):\n",
    "            A=((X - self.list_mu[k])@inv_cvar)\n",
    "            #print('dima',A)\n",
    "            B=(X-self.list_mu[k])\n",
    "            #print('dimb',B)\n",
    "            C=np.sum(np.multiply(A,B),axis=1)\n",
    "            e = np.exp( -0.5*C)\n",
    "            p = (2*np.pi)**half_len\n",
    "            #print('cccc',det_cvar)\n",
    "            list_prob.append(np.exp( -0.5*C)*(1/((2*np.pi)**half_len)*np.sqrt(det_cvar)))\n",
    "            #print(list_prob)\n",
    "        return list_prob\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        self.nb_class = len(np.unique(y))\n",
    "        self.classes = np.unique(y)\n",
    "        self.fi = self.ph(y)\n",
    "        self.list_mu = self.mu_computed(X,y)\n",
    "        self.cvar = self.covariance(X, y)\n",
    "        \n",
    "    def predict(self, X):        \n",
    "        proby=self.fi\n",
    "        #print(np.array(proby).shape)\n",
    "        list_probclass= np.array(self.prob_class(X)).T\n",
    "        predict_prob= list_probclass*proby\n",
    "        #print(list_probclass)\n",
    "        #print(predict_prob[0:2])\n",
    "        if 0 in self.classes:\n",
    "            return np.argmax(np.array(predict_prob),axis=1) \n",
    "        else:\n",
    "            return np.argmax(np.array(predict_prob),axis=1) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our model is 31.318016928657798 %\n"
     ]
    }
   ],
   "source": [
    "#Test our model with the dataset (drug)\n",
    "GDA = GaussianDiscriminantAnalysis()\n",
    "GDA.train(X_train,y_train)\n",
    "print('The accuracy of our model is {} %'\n",
    "      .format(np.sum(GDA.predict(X_test)+1==y_test+1)/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNaiveBayes:\n",
    "    '''\n",
    "    multivariate Bernoulli Naive bayes \n",
    "    This is a generative learning algorithm based on Bayes theorem \n",
    "    with an assumption of independence among the features which must be discretes\n",
    "    \n",
    "    We will train our model in this class using the function 'train' that will \n",
    "    takes the trainning data and train by computing the parameters.\n",
    "    \n",
    "    So the function predict will takes the test (unseen) data , classify and return \n",
    "    the prediction\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    #compute phi for each class    \n",
    "    def phiy(self,y): \n",
    "        phi=[]\n",
    "        for k in self.classes:\n",
    "            phi.append(np.sum(y==k)/len(y))\n",
    "        return phi\n",
    "    \n",
    "    # compute phi when the feature is 1 for each class\n",
    "    def phix1y(self,X,y):\n",
    "        phi_list=[]\n",
    "        for k in self.classes:\n",
    "            phix=[]\n",
    "            for i in range(X.shape[1]):\n",
    "                X_i=X[:,i]\n",
    "                select_y=X_i[y==k]\n",
    "                phix.append((np.sum(select_y==1)+1)/(np.sum(y==k)+self.nb_class))\n",
    "            phi_list.append(phix)\n",
    "        return phi_list\n",
    "    \n",
    "    # compute phi when the feature is 0 for each class\n",
    "    def phix0y(self,X,y):\n",
    "        phi_list=[]\n",
    "        for k in self.classes:\n",
    "            phix=[]\n",
    "            for i in range(X.shape[1]):\n",
    "                X_i=X[:,i]\n",
    "                select_y=X_i[y==k]\n",
    "                phix.append((np.sum(select_y==0)+1)/(np.sum(y==k)+self.nb_class))\n",
    "            phi_list.append(phix)\n",
    "        return phi_list\n",
    "    \n",
    "    # compute p(x|y) we are modelling the data\n",
    "    def probabxy(self,X):\n",
    "        \n",
    "        probxyk=[]\n",
    "        arr1=np.zeros_like(X)\n",
    "        for k in range(self.nb_class):\n",
    "            probxy1=[]\n",
    "            for i in range(X.shape[0]):\n",
    "                X_i=X[i,:]\n",
    "                prob=1\n",
    "                for j in range(len(X_i)):\n",
    "\n",
    "                    if X_i[j]==1:\n",
    "                        prob *=  self.fix1y[int(k)][j]\n",
    "                    else:\n",
    "                        \n",
    "                        prob *= self.fix0y[int(k)][j]\n",
    "                    arr1[i,j]=prob\n",
    "                probxy1.append(prob)\n",
    "            probxyk.append(probxy1)\n",
    "        return  probxyk \n",
    "    \n",
    "    # train the model by computing the parameter\n",
    "    def train(self, X,y):\n",
    "        self.nb_class = len(np.unique(y))\n",
    "        self.classes = np.unique(y)\n",
    "        self.fiy = self.phiy(y)\n",
    "        self.fix1y = np.array(self.phix1y(X,y))\n",
    "        self.fix0y = np.array(self.phix0y(X,y))\n",
    "    \n",
    "    # prediction of the classification\n",
    "    def predict(self,X) :\n",
    "        proby=self.fiy\n",
    "        list_probclass= self.probabxy(X)\n",
    "        \n",
    "        predict_prob=[]\n",
    "        predict_prob=np.array(list_probclass).T*proby\n",
    "        if 0 in self.classes:\n",
    "            return  np.argmax(predict_prob,axis=1)\n",
    "        else:\n",
    "            return  np.argmax(predict_prob,axis=1)+1         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our model is 29.504232164449817 %\n"
     ]
    }
   ],
   "source": [
    "#Test our model with the dataset (drug)\n",
    "NB = BernoulliNaiveBayes()\n",
    "NB.train(X_train,y_train)\n",
    "print('The accuracy of our model is {} %'\n",
    "      .format(np.sum(NB.predict(X_test)+1==y_test+1)/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    '''\n",
    "    Logistic regression for multi class\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 lr=0.0001, \n",
    "                 epoch=1,\n",
    "                 tolerence=10e-8,\n",
    "                 minibatchsize=30,\n",
    "                 lambd=0):\n",
    "        self.lr=lr\n",
    "        self.epoch=epoch\n",
    "        self.tolerence=tolerence\n",
    "        self.minibatchsize=minibatchsize\n",
    "        self.lambd = lambd\n",
    "        \n",
    "    def softmax(self,X,theta):\n",
    "        z = X@theta\n",
    "        z -= np.max(z)\n",
    "        return np.exp(z)/np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    \n",
    "    # compute the grandient     \n",
    "    def get_gradient(self, X, y, theta):\n",
    "        n, _ = X.shape\n",
    "        theta_wo_bias = theta.copy()\n",
    "        theta_wo_bias[0, :] = 0\n",
    "        \n",
    "        grad = (-1 / n) * X.T@(y - self.softmax(X, theta)) + self.lambd * theta_wo_bias\n",
    "        return grad\n",
    "     \n",
    "    #compute the loss\n",
    "    def loss(self, X, y, theta):\n",
    "        n, d = X.shape\n",
    "        y_hat = self.softmax(X, theta)\n",
    " \n",
    "        temp_theta = theta.copy()\n",
    "        temp_theta[0, :] = 0 # Not including bias in regularization\n",
    "       \n",
    "        loss = (-1 / n) * np.sum(y * np.log(y_hat)) \\\n",
    "                + (self.lambd/2)*np.sum(temp_theta*temp_theta)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def one_hot_encode(self, y):\n",
    "        '''\n",
    "        one hot encode the target\n",
    "        to be able to work with softmax\n",
    "        '''\n",
    "        n = len(y)\n",
    "        # Turn y into one-hot-labels if number of classes is greater than 2\n",
    "        y_encode = np.zeros((n, self.nb_classes))\n",
    "        y_encode[range(n), y] = 1 #numpy advanced indexing\n",
    "        y = y_encode\n",
    "        return y\n",
    "   \n",
    "    #mini batch gradient descent        \n",
    "    def fit(self,X,y):\n",
    "        '''\n",
    "        This method take tha data, \n",
    "        use minibactch gradient descent to update the weight\n",
    "        and compute the coresponding cost\n",
    "        '''\n",
    "        #make a copy of our data\n",
    "        X_copy=X.copy()\n",
    "        y_copy=y.copy()\n",
    "        self.classes = np.unique(y_copy)\n",
    "        self.nb_classes = len(self.classes)\n",
    "        \n",
    "         #add the intercept column\n",
    "        intercept=np.ones((X_copy.shape[0],1))\n",
    "        X_copy=np.concatenate((intercept,X_copy),axis=1)\n",
    "        \n",
    "        \n",
    "        #initialise the weight\n",
    "        self.theta=np.zeros((X_copy.shape[1], self.nb_classes))\n",
    "        \n",
    "        # One-hot encode y\n",
    "        y_copy = self.one_hot_encode(y_copy)\n",
    "       \n",
    "        diff=1\n",
    "        current_iter=1\n",
    "        #number of minibacth\n",
    "        minibatch = int(len(X_copy)/self.minibatchsize)\n",
    "        while (diff >= self.tolerence) and (current_iter<self.epoch) :\n",
    "            prev_theta=self.theta.copy()\n",
    "            random_vector=np.random.permutation(X_copy.shape[0])\n",
    "            X_cop=X_copy[random_vector]\n",
    "            y_cop=y_copy[random_vector]\n",
    "            for j in range(minibatch):\n",
    "                X_=X_cop[j*self.minibatchsize:(j+1)*self.minibatchsize]\n",
    "                y_=y_cop[j*self.minibatchsize:(j+1)*self.minibatchsize]\n",
    "                #compute the gradient\n",
    "                grad = self.get_gradient(X_, y_, self.theta)\n",
    "                #update the weight\n",
    "                self.theta = self.theta - self.lr*grad \n",
    "            cur_theta=self.theta.copy()\n",
    "            diff=np.linalg.norm(prev_theta - cur_theta)\n",
    "            current_iter+=1\n",
    "            #print('the loss function is ',self.loss(X_,y_,self.theta))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns probability of predictions.\n",
    "        \"\"\"\n",
    "        X_copy = X.copy()\n",
    "        intercept=np.ones((X_copy.shape[0],1))\n",
    "        X_copy=np.concatenate((intercept,X_copy), axis=1)\n",
    "\n",
    "        return self.softmax(X_copy, self.theta) \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "            gives the prediction using the softmax function\n",
    "        '''\n",
    "        prob = self.predict_proba(X)\n",
    "        y_predict = np.argmax(prob, axis=1)\n",
    "        return y_predict     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our model is 26.723095525997582 %\n"
     ]
    }
   ],
   "source": [
    "#Test our model with the dataset (drug)\n",
    "logReg = SoftmaxRegression(\n",
    "                            lr=0.1, \n",
    "                           epoch=300,\n",
    "                           tolerence=10e-8,\n",
    "                           minibatchsize=50,\n",
    "                           lambd=0\n",
    "                            )\n",
    "logReg.fit(X_train,y_train)\n",
    "print('The accuracy of our model is {} %'\n",
    "      .format(np.sum(logReg.predict(X_test)+1==y_test+1)/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2 -- We compare Gaussian Discriminant Analysis and Naive Bayes with Logistic regression using a different size of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(size,data):\n",
    "    max_words=500\n",
    "    data_copy=data.copy()\n",
    "    n, _ = data_copy.shape\n",
    " \n",
    "    size_data = int((n*size) /100)\n",
    "\n",
    "    split = size_data-20\n",
    "    drug_data=data_copy[ :size_data]\n",
    "   \n",
    "    X,y =convert_text(max_words, drug_data)\n",
    "   \n",
    "    X_train = X[:split]\n",
    "    y_train =y[:split]\n",
    "    X_test = X[split:]\n",
    "    y_test =y[split:]  \n",
    "    \n",
    "    GDA = GaussianDiscriminantAnalysis()\n",
    "    NB = BernoulliNaiveBayes()\n",
    "    logReg = SoftmaxRegression(lr=0.1, \n",
    "                           epoch=300,\n",
    "                           tolerence=10e-8,\n",
    "                           minibatchsize=50,\n",
    "                           lambd=0)\n",
    "    \n",
    "    \n",
    "    GDA.train(X_train,y_train)\n",
    "    predictionGDA = GDA.predict(X_test) +1\n",
    "    print('The accuracy for Gaussian Disciminant Analysis model is {} %'\n",
    "          .format( np.sum(predictionGDA == (y_test+1))/ len(y_test)*100))\n",
    "    \n",
    "    NB.train(X_train,y_train)\n",
    "    predictionNB = NB.predict(X_test) +1\n",
    "    print('The accuracy for Naive Bayes model is  {} %'\n",
    "          .format(np.sum(predictionNB == (y_test+1))/ len(y_test)*100))\n",
    "    \n",
    "    logReg.fit(X_train,y_train)\n",
    "    predictionlogReg = logReg.predict(X_test) + 1\n",
    "    print('The accuracy for logistic Regression model is {} %'\n",
    "          .format(np.sum(predictionlogReg == (y_test+1))/ len(y_test)*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We test the three model on 10% of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Gaussian Disciminant Analysis model is 35.0 %\n",
      "The accuracy for Naive Bayes model is  20.0 %\n",
      "The accuracy for logistic Regression model is 35.0 %\n"
     ]
    }
   ],
   "source": [
    "test(10,drug)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When we run our model on 10% of our data we can see that Gaussian Disciminant Analysis and \n",
    "logistic Regression models are performing well than Naive Bayes model \n",
    "so we can think that on 10% of our data the indepence assumption of the ferature is not statifies and the could be the reason why the model is broke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Gaussian Disciminant Analysis model is 30.0 %\n",
      "The accuracy for Naive Bayes model is  35.0 %\n",
      "The accuracy for logistic Regression model is 30.0 %\n"
     ]
    }
   ],
   "source": [
    "test(30,drug)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "On 30% of the data Naive Bayes model is performing well than Gaussian Disciminant Analysis  and \n",
    "logistic Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Gaussian Disciminant Analysis model is 35.0 %\n",
      "The accuracy for Naive Bayes model is  25.0 %\n",
      "The accuracy for logistic Regression model is 45.0 %\n"
     ]
    }
   ],
   "source": [
    "test(60,drug)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When the size of the data bigger, \n",
    "Disciminative learning algorithm does much more better than generative model\n",
    "As we can see in this case, logistic Regression model is doing better \n",
    "than Disciminant Analysis  and Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Gaussian Disciminant Analysis model is 30.0 %\n",
      "The accuracy for Naive Bayes model is  20.0 %\n",
      "The accuracy for logistic Regression model is 35.0 %\n"
     ]
    }
   ],
   "source": [
    "test(100,drug)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The remark we made when we had 60% of the data is confirm here\n",
    "When the size of the data bigger,Disciminative learning algorithm does much more better than generative model\n",
    "As we can see in this case, logistic Regression model is doing better \n",
    "than Disciminant Analysis  and Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
