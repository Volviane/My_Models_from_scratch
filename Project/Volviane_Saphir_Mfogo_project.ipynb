{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris = load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np .random.seed(0)\n",
    "\n",
    "# data = np.random.rand(200, 4)\n",
    "# data = np.round(data, decimals=0)\n",
    "# target = np.random.randint(1, 4, size=(200, 1))\n",
    "# data=np.concatenate((data,target),axis=1)\n",
    "# X_train = data[:150, :-1]\n",
    "# y_train = data[:150, -1]\n",
    "# X_test = data[150:, :-1]\n",
    "# y_test = data[150:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_train = pd.read_csv(\"Data/drugLibTrain_raw.tsv\",delimiter='\\t',encoding='utf-8')\n",
    "drug_test = pd.read_csv(\"Data/drugLibTest_raw.tsv\",delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(drug_train.shape)\n",
    "print(drug_test.shape)\n",
    "print(drug_train.shape + drug_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug=pd.concat([drug_train,drug_test], axis=0, join='outer', ignore_index=False, keys=None,\n",
    "          levels=None, names=None, verify_integrity=False, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug['Alltext']=drug['benefitsReview']#+' '+drug['benefitsReview']\\\n",
    "#+' ' +drug['commentsReview']+''+drug['sideEffectsReview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug['Alltext'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug['Alltext'] = drug['Alltext'].apply(lambda x : str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tr = drug.Alltext\n",
    "\n",
    "max_words = 500\n",
    "\n",
    "tokenizer_tr = Tokenizer(num_words=max_words)\n",
    "# tokenizer_tr = Tokenizer()\n",
    "tokenizer_tr.fit_on_texts(texts_tr)\n",
    "# data_tr = tokenizer_tr.texts_to_sequences(texts_tr)\n",
    "X = tokenizer_tr.texts_to_matrix(texts_tr, mode='tfidf')\n",
    "\n",
    "word_index = tokenizer_tr.word_index\n",
    "print('Found %s unique tokens.' % len(set(word_index))) #88582"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=drug['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(drug_train.shape)\n",
    "# print(drug_test.shape)\n",
    "# print(drug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drug['rating'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drug.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drug['sideEffects'].unique(),drug['sideEffects'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Discriminant Analysis(GDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiscriminantAnalysis:\n",
    "    def __init__(self,epsilon=10e-10):\n",
    "        self.epsilon=epsilon\n",
    "        \n",
    "    def ph(self,y):\n",
    "        phi=[]\n",
    "        for k in self.classes:#range(self.nb_class):\n",
    "            phi.append((np.sum(y==k))/len(y))\n",
    "        return phi   \n",
    "    \n",
    "    def phi(self, y):\n",
    "        return (np.sum(y==1))/len(y)\n",
    "    \n",
    "    def mu_computed(self,X,y):\n",
    "        list_of_mu=[]\n",
    "        for k in self.classes: #range(self.nb_class):\n",
    "            list_of_mu.append((np.sum(X[y==k],axis=0))/np.sum(y==k))\n",
    "            \n",
    "        return list_of_mu\n",
    "    \n",
    "    def covariance(self,X,y):\n",
    "        sigma= np.zeros((len(X),len(X)))\n",
    "        M = np.zeros_like(X)\n",
    "        for i in range(1,self.nb_class):\n",
    "            M[y== i]=self.list_mu[i]\n",
    "        return (((X-M).T)@(X-M))/len(y)\n",
    "    \n",
    "    def prob_class(self,X):\n",
    "        half_len = X.shape[1]/2\n",
    "        det_cvar = np.sqrt(np.linalg.det(self.cvar ))\n",
    "        inv_cvar = np.linalg.inv(self.cvar + (self.epsilon*np.eye(len(self.cvar))))\n",
    "        list_prob=[]\n",
    "        for i in range(self.nb_class):#range(self.nb_class):\n",
    "            A=((X - self.list_mu[i])@inv_cvar)\n",
    "            #print('dima',A)\n",
    "            B=(X-self.list_mu[i])\n",
    "            #print('dimb',B)\n",
    "            C=np.sum(np.multiply(A,B),axis=1)\n",
    "            list_prob.append(np.exp( -0.5*C)*(1/((2*np.pi)**half_len)*det_cvar))\n",
    "        #print(list_prob[0:1])\n",
    "        return list_prob\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        self.nb_class = len(np.unique(y))\n",
    "        self.classes = np.unique(y)\n",
    "        self.fi = self.ph(y)\n",
    "        self.list_mu = self.mu_computed(X,y)\n",
    "        self.cvar = self.covariance(X, y)\n",
    "        \n",
    "    def predict(self, X):        \n",
    "        proby=self.fi\n",
    "        \n",
    "        list_probclass= np.array(self.prob_class(X)).T\n",
    "        predict_prob= list_probclass*proby\n",
    "        #print(list_probclass[0:2])\n",
    "        #print(predict_prob[0:2])\n",
    "        if 0 in self.classes:\n",
    "            return np.argmax(np.array(predict_prob),axis=1) \n",
    "        else:\n",
    "            return np.argmax(np.array(predict_prob),axis=1) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDA = GaussianDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDA.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=GDA.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(prediction==y_test)/len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pred==y_test)/len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction=GDA.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(prediction == y_test) / len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np .random.seed(0)\n",
    "\n",
    "# data = np.random.rand(200, 4)\n",
    "# data = np.round(data, decimals=0)\n",
    "# target = np.random.randint(0, 3, size=(200, 1))\n",
    "# data=np.concatenate((data,target),axis=1)\n",
    "# X_train = data[:150, :-1]\n",
    "# y_train = data[:150, -1]\n",
    "# X_test = data[150:, :-1]\n",
    "# y_test = data[150:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:100,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*np.ones(100).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.((200,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = drug[['commentsReview','benefitsReview','sideEffectsReview']].apply(lambda x: \n",
    "#                                                     ' '.join(str(s) for s in x), axis=1)\n",
    "# c=CountVectorizer(max_features=5,binary=True)\n",
    "# j = c.fit_transform(text)\n",
    "# X = j.toarray()\n",
    "# y=drug['rating']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb #ipython debogeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNaiveBayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def phiy(self,y): \n",
    "        phi=[]\n",
    "        for k in self.classes:\n",
    "            phi.append(np.sum(y==k)/len(y))\n",
    "        return phi\n",
    "    \n",
    "    \n",
    "    def phix1y(self,X,y):\n",
    "        phi_list=[]\n",
    "        for k in self.classes:\n",
    "            phix=[]\n",
    "            for i in range(X.shape[1]):\n",
    "                X_i=X[:,i]\n",
    "                select_y=X_i[y==k]\n",
    "                phix.append((np.sum(select_y==1)+1)/(np.sum(y==k)+self.nb_class))\n",
    "            phi_list.append(phix)\n",
    "        return phi_list\n",
    "    \n",
    "    def phix0y(self,X,y):\n",
    "        phi_list=[]\n",
    "        for k in self.classes:\n",
    "            phix=[]\n",
    "            for i in range(X.shape[1]):\n",
    "                X_i=X[:,i]\n",
    "                select_y=X_i[y==k]\n",
    "                phix.append((np.sum(select_y==0)+1)/(np.sum(y==k)+self.nb_class))\n",
    "            phi_list.append(phix)\n",
    "        return phi_list\n",
    "    \n",
    "    def probabxy(self,X):\n",
    "        \n",
    "        probxyk=[]\n",
    "        arr1=np.zeros_like(X)\n",
    "#         ipdb.set_trace()\n",
    "        for k in range(self.nb_class):\n",
    "            probxy1=[]\n",
    "            for i in range(X.shape[0]):\n",
    "                X_i=X[i,:]\n",
    "                prob=1\n",
    "                for j in range(len(X_i)):\n",
    "\n",
    "                    if X_i[j]==1:\n",
    "                        prob *=  self.fix1y[int(k)][j]\n",
    "                    else:\n",
    "                        \n",
    "                        prob *= self.fix0y[int(k)][j]\n",
    "                    arr1[i,j]=prob\n",
    "                probxy1.append(prob)\n",
    "            #print(np.array(probxy1).shape)\n",
    "            probxyk.append(probxy1)\n",
    "        return  probxyk \n",
    "\n",
    "    def fit_train(self, X,y):\n",
    "        self.nb_class = len(np.unique(y))\n",
    "        self.classes = np.unique(y)\n",
    "        self.fiy = self.phiy(y)\n",
    "        self.fix1y = np.array(self.phix1y(X,y))\n",
    "        self.fix0y = np.array(self.phix0y(X,y))\n",
    "        #print(self.fix1y)\n",
    "        #print(self.fix0y[0:10])\n",
    "        \n",
    "    def predict(self,X) :\n",
    "        proby=self.fiy\n",
    "        list_probclass= self.probabxy(X)\n",
    "        \n",
    "        predict_prob=[]\n",
    "        predict_prob=np.array(list_probclass).T*proby\n",
    "        if 0 in self.classes:\n",
    "            return  np.argmax(predict_prob,axis=1)\n",
    "        else:\n",
    "            return  np.argmax(predict_prob,axis=1)+1         \n",
    "        \n",
    "        \n",
    "        #return  np.argmax(predict_prob,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NB = BernoulliNaiveBayes()\n",
    "NB.fit_train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.sum(prediction==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.sum(prediction==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB.fit_train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction=NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(prediction==y_test)/len(y_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('../Data/diabetes.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X is  (768, 8)\n",
      "shape of y (768, 1)\n"
     ]
    }
   ],
   "source": [
    "X=data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "       'BMI', 'DiabetesPedigreeFunction', 'Age']].values\n",
    "print('shape of X is ',X.shape)\n",
    "\n",
    "\n",
    "y=data['Outcome'].values.reshape(-1,1)\n",
    "print('shape of y',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(761, 8)\n",
      "(761, 1)\n",
      "(7, 8)\n"
     ]
    }
   ],
   "source": [
    "X_train =X[0:761]\n",
    "X_test=X[761:]\n",
    "\n",
    "print(X_train.shape)\n",
    "y_train=y[0:761].reshape(-1,1)\n",
    "y_true=y[761:]\n",
    "y=y.reshape(-1,1)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np .random.seed(0)\n",
    "\n",
    "# data = np.random.rand(200, 4)\n",
    "# data = np.round(data, decimals=0)\n",
    "# target = np.random.randint(0, 3, size=(200, 1))\n",
    "# data=np.concatenate((data,target),axis=1)\n",
    "# X_train = data[:150, :-1]\n",
    "# y_train = data[:150, -1]\n",
    "# X_test = data[150:, :-1]\n",
    "# y_test = data[150:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegresion:\n",
    "    def __init__(self, lr=0.0001, epoch=1,tolerence=10e-8,epsilon=10e-8,minibatchsize=30):\n",
    "        self.lr=lr\n",
    "        self.epoch=epoch\n",
    "        self.epsilon=epsilon\n",
    "        self.tolerence=tolerence\n",
    "        self.minibatchsize=minibatchsize\n",
    "        \n",
    "    #hypothesis : logistic sigmoid function\n",
    "    def sigmoid(self,x,W):\n",
    "        \n",
    "        vec=np.dot(x,W)\n",
    "        #print(x.shape)\n",
    "        #vec=numpy.add(vec,b)\n",
    "        #print(vec.shape)\n",
    "        #print(W.shape)\n",
    "        vec1=np.exp(vec)\n",
    "        res=vec1.T/np.sum(vec1)\n",
    "        return res.T\n",
    "    \n",
    "    \n",
    "#     def sigmoid(self,X,theta):\n",
    "#         #print('shape of X', X.shape)\n",
    "#         #print('shape pg theta', theta.shape)\n",
    "#         z = X@theta\n",
    "#         return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    # compute the grandient  \n",
    "    def get_gradientB(self,X,y,theta):\n",
    "        h   = self.sigmoid(X,theta)\n",
    "        return np.dot(X.T, (h - y))\n",
    "    \n",
    "    def get_gradientS(self,X,y,theta):\n",
    "        return (1/len(X))*np.sum(( self.sigmoid(X,theta) - y)*X)\n",
    "    \n",
    "    #compute the loss\n",
    "    def loss(self,X,y,theta):\n",
    "        return   (-y*np.log(self.sigmoid(X,theta))  - (1-y)*np.log(1- self.sigmoid(X,theta))).mean()\n",
    "            \n",
    "    #mini batch gradient descent        \n",
    "    def fit_MBGD(self,X,y):\n",
    "        \n",
    "        #make a copy of our data\n",
    "        X_copy=X.copy()\n",
    "        y_copy=y.copy()\n",
    "        \n",
    "         #add the intercept column\n",
    "        intercept=np.ones((X_copy.shape[0],1))\n",
    "        X_copy=np.concatenate((intercept,X_copy),axis=1)\n",
    "        #print('shape of the whole x',X_copy.shape)\n",
    "        \n",
    "        #initialise the weight\n",
    "        self.theta=np.zeros((X_copy.shape[1],1))\n",
    "        #print('shape of theta',theta.shape)\n",
    "        diff=1\n",
    "        current_iter=1\n",
    "        minibatch = int(len(X_copy)/self.minibatchsize)\n",
    "        #while diff >= self.tolerence current_iter<self.max_iter :    \n",
    "        for i in range(self.epoch):\n",
    "            random_vector=np.random.permutation(X_copy.shape[0])\n",
    "            #random_vectory=np.random.permutation(y_copy.shape[0])\n",
    "            X_cop=X_copy[random_vector]\n",
    "            y_cop=y_copy[random_vector]\n",
    "            #print('X_ shape',X_cop.shape)\n",
    "            #print('y_ shape',y_cop.shape)\n",
    "            #prev_theta=self.theta.copy()\n",
    "            for j in range(minibatch):\n",
    "                X_=X_cop[j*self.minibatchsize:(j+1)*self.minibatchsize]\n",
    "                y_=y_cop[j*self.minibatchsize:(j+1)*self.minibatchsize]\n",
    "#                     print('X_ shape',X_.shape)\n",
    "#                     print('y_ shape',y_.shape)\n",
    "                #compute the gradient\n",
    "                grad = self.get_gradientS(X_, y_, self.theta)\n",
    "\n",
    "                self.theta = self.theta + self.lr*grad.reshape(-1,1)\n",
    "                #print('shape of theta',self.theta.shape)\n",
    "            #cur_theta=self.theta.copy()\n",
    "            #diff=np.linalg.norm(prev_theta - cur_theta)\n",
    "            #current_iter+=1\n",
    "            #print('my update',self.theta)\n",
    "            #print the value of the loss\n",
    "            print('the loss function is ',self.loss(X_[j],y_[j],self.theta))\n",
    "\n",
    "        \n",
    "\n",
    "    def predict(self,X):\n",
    "        X_copy=X.copy()\n",
    "        \n",
    "        #print('shape of theta',self.theta.shape)\n",
    "        intercept=np.ones((X_copy.shape[0],1))\n",
    "        #print('shape of x copy' ,X_copy.shape)\n",
    "        #print('shape of intercept' ,intercept.shape)\n",
    "        X_copy=np.concatenate((intercept, X_copy), axis=1)\n",
    "        #print('shape',X_copy.shape)\n",
    "        \n",
    "        return self.sigmoid(X_copy,self.theta)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegresion( lr=0.0000001, \n",
    "                           epoch=30,\n",
    "                           tolerence=10e-8,\n",
    "                           epsilon=10e-8,\n",
    "                           minibatchsize=50,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss function is  nan\n",
      "the loss function is  nan\n",
      "the loss function is  nan\n",
      "the loss function is  inf\n",
      "the loss function is  nan\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  nan\n",
      "the loss function is  nan\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  nan\n",
      "the loss function is  inf\n",
      "the loss function is  nan\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n",
      "the loss function is  nan\n",
      "the loss function is  nan\n",
      "the loss function is  inf\n",
      "the loss function is  inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "logReg.fit_MBGD(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=logReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09293041],\n",
       "       [0.2453588 ],\n",
       "       [0.03891148],\n",
       "       [0.15910034],\n",
       "       [0.08304592],\n",
       "       [0.18100121],\n",
       "       [0.19965183]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2756644126341665"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean(np.sum((predict - y_true)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = np.array([0, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoding:\n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y_enc = (np.arange(np.max(y) + 1) == y[:, None]).astype(float)\n",
    "\n",
    "print('one-hot encoding:\\n', y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.1, 0.5],\n",
    "              [1.1, 2.3],\n",
    "              [-1.1, -2.3],\n",
    "              [-1.5, -2.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.1, 0.2, 0.3]])\n",
    "\n",
    "bias = np.array([0.01, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs X:\n",
      " [[ 0.1  0.5]\n",
      " [ 1.1  2.3]\n",
      " [-1.1 -2.3]\n",
      " [-1.5 -2.5]]\n",
      "\n",
      "Weights W:\n",
      " [[0.1 0.2 0.3]\n",
      " [0.1 0.2 0.3]]\n",
      "\n",
      "bias:\n",
      " [0.01 0.1  0.1 ]\n"
     ]
    }
   ],
   "source": [
    "print('Inputs X:\\n', X)\n",
    "print('\\nWeights W:\\n', W)\n",
    "print('\\nbias:\\n', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net input:\n",
      " [[ 0.07  0.22  0.28]\n",
      " [ 0.35  0.78  1.12]\n",
      " [-0.33 -0.58 -0.92]\n",
      " [-0.39 -0.7  -1.1 ]]\n"
     ]
    }
   ],
   "source": [
    "def net_input(X, W, b):\n",
    "    return (X.dot(W) + b)\n",
    "\n",
    "net_in = net_input(X, W, bias)\n",
    "print('net input:\\n', net_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax:\n",
      " [[0.29450637 0.34216758 0.36332605]\n",
      " [0.21290077 0.32728332 0.45981591]\n",
      " [0.42860913 0.33380113 0.23758974]\n",
      " [0.44941979 0.32962558 0.22095463]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "smax = softmax(net_in)\n",
    "print('softmax:\\n', smax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class labels:  [2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "def to_classlabel(z):\n",
    "    return z.argmax(axis=1)\n",
    "\n",
    "print('predicted class labels: ', to_classlabel(smax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: [1.22245465 1.11692907 1.43720989 1.50979788]\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(output, y_target):\n",
    "    return - np.sum(np.log(output) * (y_target), axis=1)\n",
    "\n",
    "xent = cross_entropy(smax, y_enc)\n",
    "print('Cross Entropy:', xent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  1.3215978715930938\n"
     ]
    }
   ],
   "source": [
    "def cost(output, y_target):\n",
    "    return np.mean(cross_entropy(output, y_target))\n",
    "\n",
    "J_cost = cost(smax, y_enc)\n",
    "print('Cost: ', J_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
